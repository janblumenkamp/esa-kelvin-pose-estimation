{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing Samples and Labels of the SPEED Dataset\n",
    "\n",
    "This notebook helps to inspect the SPEED dataset. You can see samples from the dataset, with the corresponding ground truth labels visualized as projected axes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0' # 1: RTX, 0: Titan\n",
    "\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Input, Activation, AveragePooling2D, concatenate, Conv2D, MaxPooling2D, Conv2DTranspose, Dense, BatchNormalization, Dropout, LeakyReLU\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras import backend as K\n",
    "import tensorflow\n",
    "from tensorflow.keras.applications.vgg19 import preprocess_input\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import tensorflow as tf\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "from tensorflow.keras.preprocessing import image as keras_image\n",
    "#%matplotlib notebook\n",
    "from utils import *\n",
    "from tensorflow.keras.utils import Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only for rtx\n",
    "config = tf.ConfigProto(allow_soft_placement=True)\n",
    "config.gpu_options.allow_growth = True\n",
    "K.set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_root_dir = './speed'\n",
    "dataset = SatellitePoseEstimationDataset(root_dir=dataset_root_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = 4\n",
    "cols = 2\n",
    "#%matplotlib notebook\n",
    "%matplotlib inline\n",
    "def drawBlob(img, pos, size=3, color=[255, 0, 0]):\n",
    "    for y in range(pos[1] - size, pos[1] + size):\n",
    "        for x in range(pos[0] - size, pos[0] + size):\n",
    "            img[y][x] = color\n",
    "\n",
    "# 1) 8 Kantenpunkte bestimmen\n",
    "# 2) 8 entspr. Flächen bestimmenn\n",
    "# 3) 8 Vektor zwischen Kameraprojektion und 3D Punkt bestimmen\n",
    "# 4) Überprüfen, ob die 8 Vektoren irgendeine der 8 Flächen durchschneiden. Wenn ja: Punkt verwerfen!\n",
    "\n",
    "for i in range(0, 1):\n",
    "    img = np.array(dataset.get_image(i))\n",
    "    q, r = dataset.get_pose(i)\n",
    "    xa, ya, visible = projectModel(q, r)\n",
    "    for x, y, v in zip(xa, ya, visible):\n",
    "        if v and x >= 0.0 and y >= 0.0 and x <= Camera.nu and y <= Camera.nv:\n",
    "            drawBlob(img, (int(x), int(y)))\n",
    "    \n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(img)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_q = np.array([0]*4, dtype=np.float64)\n",
    "avg_r = np.array([0]*3, dtype=np.float64)\n",
    "max_z = 0\n",
    "for i in range(12000):\n",
    "    q, r = dataset.get_pose(i)\n",
    "    avg_q += q\n",
    "    avg_r += r\n",
    "    if r[2] > max_z:\n",
    "        max_z = r[2]\n",
    "avg_q /= 12000\n",
    "avg_r /= 12000\n",
    "print(list(avg_q), list(avg_r), max_z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project a 3D point (0, 0, 1000.0) onto the image plane.\n",
    "# We use this to draw a line sticking out of the nose\n",
    " \n",
    " \n",
    "(nose_end_point2D, jacobian) = cv2.projectPoints(np.array([(1.0, 1.0, 1.0)]), rotation_vector, translation_vector, Camera.K, None)\n",
    " \n",
    "for p in image_points:\n",
    "    cv2.circle(img, (int(p[0]), int(p[1])), 5, (0,0,255), -1)\n",
    " \n",
    " \n",
    "p1 = ( int(image_points[0][0]), int(image_points[0][1]))\n",
    "p2 = ( int(nose_end_point2D[0][0][0]), int(nose_end_point2D[0][0][1]))\n",
    " \n",
    "cv2.line(img, p1, p2, (255,0,0), 2)\n",
    " \n",
    "# Display image\n",
    "plt.figure(figsize=(20,20))\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tensorflow.keras.backend.set_learning_phase(0)\n",
    "pretrained_model = tensorflow.keras.applications.vgg19.VGG19(weights=\"imagenet\", include_top=False,\n",
    "                                                          input_shape=(600, 960, 3))\n",
    "\n",
    "def create_stage(inp, filters):\n",
    "    c = Conv2D(filters=64, kernel_size=7, strides=1, padding=\"same\")(inp)\n",
    "    c = BatchNormalization()(c)\n",
    "    c = Activation(\"relu\")(c)\n",
    "    \n",
    "    for i in range(4):\n",
    "        c = Conv2D(filters=64, kernel_size=7, strides=1, padding=\"same\")(c)\n",
    "        c = BatchNormalization()(c)\n",
    "        c = Activation(\"relu\")(c)\n",
    "\n",
    "    c = Conv2D(filters=64, kernel_size=1, strides=1, padding=\"same\")(c)\n",
    "    c = BatchNormalization()(c)\n",
    "    c = Activation(\"relu\")(c)\n",
    "    return Conv2D(filters=filters, kernel_size=1, strides=1, padding=\"same\")(c)\n",
    "\n",
    "numBeliefMap = 8\n",
    "    \n",
    "# Adding new trainable hidden and output layers to the model\n",
    "#tensorflow.keras.backend.set_learning_phase(1)\n",
    "\n",
    "inp = pretrained_model.input\n",
    "inp_avg = AveragePooling2D(pool_size=9, strides=8, padding=\"same\")(inp)\n",
    "\n",
    "c = pretrained_model.layers[13].output\n",
    "c = Conv2D(filters=128, kernel_size=3, strides=1, padding=\"same\")(c)\n",
    "c = BatchNormalization()(c)\n",
    "c = Activation(\"relu\")(c)\n",
    "\n",
    "pre = Conv2D(filters=128, kernel_size=3, strides=1, padding=\"same\")(c)\n",
    "pre = BatchNormalization()(pre)\n",
    "pre = Activation(\"relu\")(pre)\n",
    "\n",
    "s1 = Conv2D(filters=128, kernel_size=1, strides=1, padding=\"same\")(pre)\n",
    "stages = [Conv2D(filters=numBeliefMap, kernel_size=1, strides=1, padding=\"same\")(s1)]\n",
    "\n",
    "for i in range(5):\n",
    "    stages.append(create_stage(concatenate([pre, stages[-1], inp_avg], axis=3), numBeliefMap))\n",
    "\n",
    "model = tensorflow.keras.models.Model(inputs=inp, outputs=stages)\n",
    "model.compile(loss=\"mse\", optimizer=Adam(lr=1e-3))\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(\n",
    "    model,\n",
    "    to_file='model.png',\n",
    "    show_shapes=False,\n",
    "    show_layer_names=True,\n",
    "    rankdir='TB'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imgaug import augmenters as iaa\n",
    "\n",
    "class KerasDataGenerator2(Sequence):\n",
    "\n",
    "    \"\"\" DataGenerator for Keras to be used with fit_generator (https://keras.io/models/sequential/#fit_generator)\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 preprocessor,\n",
    "                 label_list,\n",
    "                 speed_root,\n",
    "                 label_size,\n",
    "                 batch_size=32,\n",
    "                 dim=(224, 224),\n",
    "                 shuffle=True,\n",
    "                 output_scale=8,\n",
    "                 n_output_vertices=8,\n",
    "                 stages=6):\n",
    "\n",
    "        # loading dataset\n",
    "        self.image_root = os.path.join(speed_root, 'images', 'train')\n",
    "\n",
    "        # Initialization\n",
    "        self.preprocessor = preprocessor\n",
    "        self.dim = dim\n",
    "        self.batch_size = batch_size\n",
    "        self.labels = self.labels = {label['filename']: {'q': label['q_vbs2tango'], 'r': label['r_Vo2To_vbs_true']}\n",
    "                                     for label in label_list}\n",
    "        self.list_IDs = [label['filename'] for label in label_list]\n",
    "        self.shuffle = shuffle\n",
    "        self.label_size = label_size\n",
    "        self.indexes = None\n",
    "        self.output_scale = output_scale\n",
    "        self.n_output_vertices = n_output_vertices\n",
    "        self.stages = stages\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        \"\"\" Denotes the number of batches per epoch. \"\"\"\n",
    "\n",
    "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        \"\"\" Generate one batch of data \"\"\"\n",
    "\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "\n",
    "        # Generate data\n",
    "        return self.__data_generation(list_IDs_temp)\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "\n",
    "        \"\"\" Updates indexes after each epoch \"\"\"\n",
    "\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def drawBlob(self, img, pos, sigma=3):\n",
    "        # https://github.com/NVlabs/Deep_Object_Pose/blob/master/src/training/train.py#L851\n",
    "        w = int(sigma*3)\n",
    "        if pos[0]-w>=0 and pos[0]+w<img.shape[0] and pos[1]-w>=0 and pos[1]+w<img.shape[1]:\n",
    "            for i in range(int(pos[0])-w, int(pos[0])+w):\n",
    "                for j in range(int(pos[1])-w, int(pos[1])+w):\n",
    "                    img[i,j] = np.exp(-(((i - pos[0])**2 + (j - pos[1])**2)/(2*(sigma**2))))\n",
    "\n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "\n",
    "        \"\"\" Generates data containing batch_size samples \"\"\"\n",
    "\n",
    "        # Initialization\n",
    "        imgs = np.empty((self.batch_size, *self.dim, 3))\n",
    "        masks = np.zeros((self.batch_size, int(self.dim[0] / self.output_scale), int(self.dim[1] / self.output_scale), self.n_output_vertices), dtype=np.float)\n",
    "\n",
    "        seq = iaa.SomeOf((0, 3),\n",
    "            [\n",
    "                iaa.OneOf([\n",
    "                    iaa.GaussianBlur((0, 3.0)), # blur images with a sigma between 0 and 3.0\n",
    "                    iaa.AverageBlur(k=(2, 7)), # blur image using local means with kernel sizes between 2 and 7\n",
    "                    #iaa.MedianBlur(k=(3, 11)), # blur image using local medians with kernel sizes between 2 and 7\n",
    "                ]),\n",
    "                iaa.Sharpen(alpha=(0, 1.0), lightness=(0.75, 1.5)), # sharpen images\n",
    "                # search either for all edges or for directed edges,\n",
    "                # blend the result with the original image using a blobby mask\n",
    "                iaa.SimplexNoiseAlpha(iaa.OneOf([\n",
    "                    iaa.EdgeDetect(alpha=(0.5, 1.0)),\n",
    "                    iaa.DirectedEdgeDetect(alpha=(0.5, 1.0), direction=(0.0, 1.0)),\n",
    "                ])),\n",
    "                iaa.AdditiveGaussianNoise(loc=0, scale=(0.0, 0.05*255)), # add gaussian noise to images\n",
    "                iaa.OneOf([\n",
    "                    iaa.Dropout((0.01, 0.1)), # randomly remove up to 10% of the pixels\n",
    "                    iaa.CoarseDropout((0.03, 0.15), size_percent=(0.02, 0.05)),\n",
    "                ]),\n",
    "                iaa.Add((-10, 10)), # change brightness of images (by -10 to 10 of original value)\n",
    "                iaa.OneOf([\n",
    "                    iaa.Multiply((0.5, 1.5)),\n",
    "                    iaa.FrequencyNoiseAlpha(\n",
    "                        exponent=(-4, 0),\n",
    "                        first=iaa.Multiply((0.5, 1.5)),\n",
    "                        second=iaa.ContrastNormalization((0.5, 2.0))\n",
    "                    )\n",
    "                ]),\n",
    "                iaa.ContrastNormalization((0.5, 2.0)), # improve or worsen the contrast\n",
    "            ],\n",
    "            random_order=True\n",
    "        ).to_deterministic()\n",
    "\n",
    "        # Generate data\n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            img_path = os.path.join(self.image_root, ID)\n",
    "            img = keras_image.load_img(img_path, target_size=self.dim) #, color_mode = \"grayscale\")\n",
    "            img = keras_image.img_to_array(img)\n",
    "            img_aug = seq.augment_image(img)\n",
    "            imgs[i] = self.preprocessor(img_aug)\n",
    "\n",
    "            q, r = self.labels[ID]['q'], self.labels[ID]['r']\n",
    "            xa, ya, visibles = projectModel(q, r)\n",
    "            for j, (x, y, visible) in enumerate(zip(xa, ya, visibles)):\n",
    "                if j >= self.n_output_vertices:\n",
    "                    break\n",
    "                \n",
    "                x /= (Camera.nu * self.output_scale)\n",
    "                y /= (Camera.nv * self.output_scale)\n",
    "                if visible and x >= 0.0 and y >= 0.0 and x <= 1.0 and y <= 1.0:\n",
    "                    x_s, y_s = int(x * self.dim[1]), int(y * self.dim[0])\n",
    "                    self.drawBlob(masks[i][...,j], (y_s, x_s), self.label_size)\n",
    "\n",
    "                #masks[i][0][0] = 0\n",
    "        return imgs, [masks*255 for i in range(self.stages)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "# Setting up parameters\n",
    "params = {'dim': (600, 960),\n",
    "          'batch_size': 6,\n",
    "          'label_size': 1.5,\n",
    "          'shuffle': True,\n",
    "          'output_scale': 8,\n",
    "          'n_output_vertices': 8,\n",
    "          'stages': 6}\n",
    "\n",
    "# Loading and splitting dataset\n",
    "with open(os.path.join(dataset_root_dir, 'train' + '.json'), 'r') as f:\n",
    "    label_list = json.load(f)\n",
    "    random.Random(4).shuffle(label_list)\n",
    "\n",
    "train_labels = label_list[:int(len(label_list)*.8)]\n",
    "validation_labels = label_list[int(len(label_list)*.8):]\n",
    "\n",
    "# Data generators for training and validation\n",
    "training_generator = KerasDataGenerator2(preprocess_input, train_labels, dataset_root_dir, **params)\n",
    "validation_generator = KerasDataGenerator2(preprocess_input, validation_labels, dataset_root_dir, **params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for imgs, masks in training_generator:\n",
    "    print(imgs.shape, len(masks), masks[0].shape)\n",
    "    for img, mask in zip(imgs, masks[0]):        \n",
    "        # plot with various axes scales\n",
    "\n",
    "        print(img.min(), img.max(), np.asarray(mask).min(), np.asarray(mask).max())\n",
    "        plt.figure(figsize=(20, 20))\n",
    "\n",
    "        plt.subplot(121)\n",
    "        plt.imshow(img[...,0], cmap='gray')\n",
    "\n",
    "        plt.subplot(122)\n",
    "        m = np.zeros(mask.shape[:2], dtype=np.float)\n",
    "        #for i in range(8):\n",
    "        #    m += mask[...,i]\n",
    "        m += mask[...,1]\n",
    "        plt.imshow(m, cmap='gray')\n",
    "        plt.show()\n",
    "        #break\n",
    "    #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(label_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(\"m7.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(\"m7_weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(\"m7_weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_model = \"m8.h5\"\n",
    "checkpoint = ModelCheckpoint(current_model,save_best_only=True, verbose=1, monitor=\"val_loss\")\n",
    "reduce = ReduceLROnPlateau(factor=0.5, patience=5, monitor='val_loss')\n",
    "earlyStopping = EarlyStopping(patience=10, verbose=1,monitor=\"val_loss\")\n",
    "history = model.fit_generator(\n",
    "    generator=training_generator,\n",
    "    validation_data=validation_generator,\n",
    "    use_multiprocessing=False, # Only works if training data is loaded into RAM from HDF\n",
    "    workers=8,\n",
    "    callbacks=[earlyStopping, checkpoint],# reduce],\n",
    "    epochs=10000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"m7_final.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "index = 0\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    " \n",
    "# Read Image\n",
    "img = np.array(dataset.get_image(index))\n",
    "q, r = dataset.get_pose(index)\n",
    "xa, ya, visible = projectModel(q, r)\n",
    "size = img.shape\n",
    "\n",
    "model_points_all, _ = getSatelliteModel()\n",
    "image_points_all = np.stack((xa, ya), axis=1)\n",
    "\n",
    "model_points = model_points_all#[visible]\n",
    "image_points = image_points_all#[visible]\n",
    "#np.random.shuffle(image_points)\n",
    "print(visible)\n",
    "print(model_points)\n",
    "print(image_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyrr import Quaternion\n",
    "\n",
    "def points_to_pose(points_3d, points_2d, camera_matrix=Camera.K):\n",
    "    def convert_rvec_to_quaternion(rvec):\n",
    "        '''Convert rvec (which is log quaternion) to quaternion'''\n",
    "        theta = np.sqrt(rvec[0] * rvec[0] + rvec[1] * rvec[1] + rvec[2] * rvec[2])  # in radians\n",
    "        raxis = [rvec[0] / theta, rvec[1] / theta, rvec[2] / theta]\n",
    "\n",
    "        # pyrr's Quaternion (order is XYZW), https://pyrr.readthedocs.io/en/latest/oo_api_quaternion.html\n",
    "        return np.roll(Quaternion.from_axis_rotation(raxis, theta), 1) # change order to wxyz\n",
    "\n",
    "    (success, rotation_vector, translation_vector, outliners) = cv2.solvePnPRansac(\n",
    "        points_3d, points_2d, camera_matrix, None, iterationsCount=200, reprojectionError=38)\n",
    "    #(success, rotation_vector, translation_vector) = cv2.solvePnP(points_3d, points_2d, camera_matrix, None)\n",
    "    #print(outliners)\n",
    "    if success:\n",
    "        location = list(translation_vector[...,0])\n",
    "        quaternion = convert_rvec_to_quaternion(rotation_vector)\n",
    "\n",
    "        projected_points, _ = cv2.projectPoints(points_3d, rotation_vector, translation_vector, camera_matrix, None)\n",
    "        projected_points = np.squeeze(projected_points)\n",
    "\n",
    "        # If the location.Z is negative or object is behind the camera then flip both location and rotation\n",
    "        x, y, z = location\n",
    "        if z < 0:\n",
    "            print(\"neg\")\n",
    "            # Get the opposite location\n",
    "            location = [-x, -y, -z]\n",
    "\n",
    "            # Change the rotation by 180 degree\n",
    "            rotate_angle = np.pi\n",
    "            rotate_quaternion = Quaternion.from_axis_rotation(location, rotate_angle)\n",
    "            quaternion = rotate_quaternion.cross(quaternion)\n",
    "\n",
    "        return quaternion, location, projected_points\n",
    "    return [], [], []\n",
    "quat, loc, points = points_to_pose(model_points, image_points)\n",
    "print(q, r)\n",
    "print(quat, loc)\n",
    "print(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.transform import Rotation as R\n",
    "print(R.from_quat(quaternion).as_euler('zyx', degrees=True))\n",
    "print(R.from_quat(q).as_euler('zyx', degrees=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points_2d = np.asarray([\n",
    " [ 752., 1008.],\n",
    " [ 608.,  304.],\n",
    " [ 880.,  960.],\n",
    " [ 688.,  368.],\n",
    " [1040.,  656.],\n",
    " [1056.,  672.],\n",
    " [ 848.,  320.],\n",
    " [ 880.,  944.],\n",
    " [1040.,  656.]], dtype=np.float32)\n",
    "\n",
    "points_3d = np.asarray([\n",
    " [ 0.375, -0.4,    0.32 ],\n",
    " [-0.375, -0.4,    0.32 ],\n",
    " [-0.375,  0.3,    0.   ],\n",
    " [-0.375,  0.3,    0.   ],\n",
    " [-0.375,  0.3,    0.   ],\n",
    " [ 0.375,  0.3,    0.   ],\n",
    " [ 0.375, -0.3,    0.   ],\n",
    " [-0.375, -0.3,    0.   ],\n",
    " [-0.375, -0.3,    0.   ]], dtype=np.float32)\n",
    "print(points_2d)\n",
    "print(points_3d)\n",
    "quat_res, trans_res, points = points_to_pose(points_3d, points_2d)\n",
    "print(quat_res, trans_res)\n",
    "print(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = 4\n",
    "cols = 2\n",
    "\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(12, 12))\n",
    "for i in range(rows):\n",
    "    for j in range(cols):\n",
    "        dataset.visualize(i * rows + j, ax=axes[i][j])\n",
    "        axes[i][j].axis('off')\n",
    "fig.tight_layout() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "def extract_maxima(belief, area=5):\n",
    "    b = belief.copy()\n",
    "    maxima = []\n",
    "    maxima_vals = []\n",
    "    for i in range(3):\n",
    "        pmax = np.unravel_index(b.argmax(), b.shape)\n",
    "        pmax_val = b[pmax[0]][pmax[1]]\n",
    "        if i > 0 and pmax_val < maxima_vals[-1] * 0.5 or pmax_val < 30.0:\n",
    "        #if pmax_val < 100.0:\n",
    "            break\n",
    "        maxima_vals.append(pmax_val)\n",
    "        b[max(0,pmax[0]-area):min(b.shape[0], pmax[0]+area):, max(0,pmax[1]-area):min(b.shape[1], pmax[1]+area):] = 0\n",
    "        maxima.append(pmax)\n",
    "    return np.asarray(maxima)\n",
    "\n",
    "def predict_pose(model, img, mask=None, debug=True):\n",
    "    camera_matrix_modified = Camera.K.copy()\n",
    "    camera_matrix_modified[0][2] = params['dim'][0] / 2\n",
    "    camera_matrix_modified[1][2] = params['dim'][1] / 2\n",
    "    model_points_all, _ = getSatelliteModel()\n",
    "\n",
    "    pred = model.predict(np.asarray([img]))\n",
    "\n",
    "    if debug:\n",
    "        plt.figure(figsize=(20, 40))\n",
    "\n",
    "    points_3d = []\n",
    "    points_2d = []\n",
    "    for i, p3d in enumerate(model_points_all):\n",
    "        p = pred[-1][0][...,i]\n",
    "        if debug:\n",
    "            plt.subplot(8,3,i * 3 + 1)\n",
    "            plt.imshow((img + 128).astype(np.uint8), cmap='gray')\n",
    "            ax = plt.subplot(8,3,i * 3 + 2)\n",
    "            ax.imshow(p)\n",
    "        maxima = extract_maxima(p)\n",
    "        if debug and mask is not None:\n",
    "            axmask = plt.subplot(8,3,i * 3 + 3)\n",
    "            axmask.imshow(mask[...,i])\n",
    "        for m in maxima:\n",
    "            points_3d.append(p3d)\n",
    "            points_2d.append([m[1]*8*2, m[0]*8*2])\n",
    "            if debug:\n",
    "                ax.add_patch(Rectangle((m[1], m[0]),1,1,linewidth=5,edgecolor='r',facecolor='none'))\n",
    "                if mask is not None:\n",
    "                    axmask.add_patch(Rectangle((m[1], m[0]),1,1,linewidth=5,edgecolor='r',facecolor='none'))\n",
    "            #break\n",
    "    points_3d = np.array(points_3d, dtype=np.float32)\n",
    "    points_2d = np.array(points_2d, dtype=np.float32)\n",
    "    if len(points_2d) < 4:\n",
    "        return np.array([0]*4), np.array([0]*3)\n",
    "    \n",
    "    quat_res, trans_res, points = points_to_pose(points_3d, points_2d, camera_matrix_modified)\n",
    "    if debug:\n",
    "        plt.show()\n",
    "        print(\"orig\", points_2d)\n",
    "        print(\"model\", points_3d)\n",
    "        print(\"proj\", points)\n",
    "\n",
    "        aximgproj = plt.subplot(111)\n",
    "        aximgproj.imshow((img + 128).astype(np.uint8), cmap='gray')\n",
    "        for p in points:\n",
    "            aximgproj.add_patch(Rectangle((p[0]/2, p[1]/2),3,3,linewidth=2,edgecolor='r',facecolor='none'))\n",
    "        plt.show()\n",
    "    return quat_res, trans_res\n",
    "    #print(\"res\", quat_res, trans_res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for imgs, masks in validation_generator:\n",
    "    for img, mask in zip(imgs, masks[0]):        \n",
    "        try:\n",
    "            quat_res, trans_res = predict_pose(model, img, mask)\n",
    "        except cv2.error as e:\n",
    "            continue\n",
    "        print(\"res\", quat_res, trans_res)\n",
    "        print(\"=\"*30)\n",
    "        #break\n",
    "    #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from submission import SubmissionWriter\n",
    "\n",
    "def evaluate(model, dataset, append_submission, dataset_root):\n",
    "\n",
    "    \"\"\" Running evaluation on test set, appending results to a submission. \"\"\"\n",
    "\n",
    "    with open(os.path.join(dataset_root, dataset + '.json'), 'r') as f:\n",
    "        image_list = json.load(f)\n",
    "\n",
    "    print('Running evaluation on {} set...'.format(dataset))\n",
    "\n",
    "    err1 = 0\n",
    "    err2 = 0\n",
    "    err3 = 0\n",
    "    err4 = 0\n",
    "    for i, img in enumerate(image_list):\n",
    "        print(\"index\", i)\n",
    "        img_path = os.path.join(dataset_root, 'images', dataset, img['filename'])\n",
    "        \n",
    "        img_raw = keras_image.load_img(img_path, target_size=(600, 960)) #, color_mode = \"grayscale\")\n",
    "        img_raw = keras_image.img_to_array(img_raw)\n",
    "        img_proc = preprocess_input(img_raw)\n",
    "        \n",
    "        try:\n",
    "            quat_res, trans_res = predict_pose(model, img_proc, debug=False)\n",
    "            print(quat_res, trans_res)\n",
    "            if len(quat_res) == 0 or len(trans_res) == 0:\n",
    "                append_submission(img['filename'], [0]*4, [0,0,3])\n",
    "                err1 += 1\n",
    "            elif (np.array(trans_res) == 0).all():\n",
    "                append_submission(img['filename'], [0]*4, [0,0,10])\n",
    "                err4 += 1\n",
    "            elif trans_res[2] > 60:\n",
    "                append_submission(img['filename'], [0]*4, [0,0,40])\n",
    "                err2 += 1\n",
    "            else:\n",
    "                append_submission(img['filename'], quat_res, trans_res)\n",
    "        except cv2.error as e:\n",
    "            append_submission(img['filename'], [0]*4, [0,0,10])\n",
    "            err3 += 1\n",
    "    print(\"Err amount\", err1, err2, err3)\n",
    "            \n",
    "submission = SubmissionWriter()\n",
    "evaluate(model, 'test', submission.append_test, dataset_root_dir)\n",
    "evaluate(model, 'real_test', submission.append_real_test, dataset_root_dir)\n",
    "submission.export(suffix='keras_example')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
