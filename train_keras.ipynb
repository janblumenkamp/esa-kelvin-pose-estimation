{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing Samples and Labels of the SPEED Dataset\n",
    "\n",
    "This notebook helps to inspect the SPEED dataset. You can see samples from the dataset, with the corresponding ground truth labels visualized as projected axes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.preprocessing import image as keras_image\n",
    "#%matplotlib notebook\n",
    "from utils import *\n",
    "from keras_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_root_dir = './speed'\n",
    "dataset = SatellitePoseEstimationDataset(root_dir=dataset_root_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = 4\n",
    "cols = 2\n",
    "#%matplotlib notebook\n",
    "%matplotlib inline\n",
    "def drawBlob(img, pos, size=3, color=[255, 0, 0]):\n",
    "    for y in range(pos[1] - size, pos[1] + size):\n",
    "        for x in range(pos[0] - size, pos[0] + size):\n",
    "            img[y][x] = color\n",
    "\n",
    "# 1) 8 Kantenpunkte bestimmen\n",
    "# 2) 8 entspr. FlÃ¤chen bestimmenn\n",
    "# 3) 8 Vektor zwischen Kameraprojektion und 3D Punkt bestimmen\n",
    "# 4) ÃœberprÃ¼fen, ob die 8 Vektoren irgendeine der 8 FlÃ¤chen durchschneiden. Wenn ja: Punkt verwerfen!\n",
    "\n",
    "for i in range(0, 1):\n",
    "    img = np.array(dataset.get_image(i))\n",
    "    q, r = dataset.get_pose(i)\n",
    "    xa, ya, visible = projectModel(q, r)\n",
    "    for x, y, v in zip(xa, ya, visible):\n",
    "        if v and x >= 0.0 and y >= 0.0 and x <= Camera.nu and y <= Camera.nv:\n",
    "            drawBlob(img, (int(x), int(y)))\n",
    "    \n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(img)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up parameters\n",
    "params = {'dim': (480, 640),\n",
    "          'batch_size': 8,\n",
    "          'label_size': 3,\n",
    "          'shuffle': True}\n",
    "\n",
    "# Loading and splitting dataset\n",
    "with open(os.path.join(dataset_root_dir, 'train' + '.json'), 'r') as f:\n",
    "    label_list = json.load(f)\n",
    "train_labels = label_list[:int(len(label_list)*.8)]\n",
    "validation_labels = label_list[int(len(label_list)*.8):]\n",
    "\n",
    "# Data generators for training and validation\n",
    "training_generator = KerasDataGenerator(train_labels, dataset_root_dir, **params)\n",
    "validation_generator = KerasDataGenerator(validation_labels, dataset_root_dir, **params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for imgs, masks in training_generator:\n",
    "    print(imgs.shape, masks.shape)\n",
    "    for img, mask in zip(imgs, masks):        \n",
    "        # plot with various axes scales\n",
    "        plt.figure(figsize=(20, 20))\n",
    "\n",
    "        plt.subplot(121)\n",
    "        plt.imshow(img.astype(np.uint8)[...,0], cmap='gray')\n",
    "\n",
    "        plt.subplot(122)\n",
    "        plt.imshow(mask[...,0], cmap='gray')\n",
    "        plt.show()\n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, concatenate, Conv2D, MaxPooling2D, Conv2DTranspose, Dense, BatchNormalization, Dropout, LeakyReLU\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_distance_loss(y_true, y_pred, smooth=100):\n",
    "    \"\"\"\n",
    "    Jaccard = (|X & Y|)/ (|X|+ |Y| - |X & Y|)\n",
    "            = sum(|A*B|)/(sum(|A|)+sum(|B|)-sum(|A*B|))\n",
    "    \n",
    "    The jaccard distance loss is usefull for unbalanced datasets. This has been\n",
    "    shifted so it converges on 0 and is smoothed to avoid exploding or disapearing\n",
    "    gradient.\n",
    "    \n",
    "    Ref: https://en.wikipedia.org/wiki/Jaccard_index\n",
    "    \n",
    "    @url: https://gist.github.com/wassname/f1452b748efcbeb4cb9b1d059dce6f96\n",
    "    @author: wassname\n",
    "    \"\"\"\n",
    "    intersection = K.sum(K.abs(y_true * y_pred), axis=-1)\n",
    "    sum_ = K.sum(K.abs(y_true) + K.abs(y_pred), axis=-1)\n",
    "    jac = (intersection + smooth) / (sum_ - intersection + smooth)\n",
    "    return (1 - jac) * smooth\n",
    "\n",
    "def soft_dice_loss(y_true, y_pred, epsilon=1e-6): \n",
    "    ''' \n",
    "    Soft dice loss calculation for arbitrary batch size, number of classes, and number of spatial dimensions.\n",
    "    Assumes the `channels_last` format.\n",
    "  \n",
    "    # Arguments\n",
    "        y_true: b x X x Y( x Z...) x c One hot encoding of ground truth\n",
    "        y_pred: b x X x Y( x Z...) x c Network output, must sum to 1 over c channel (such as after softmax) \n",
    "        epsilon: Used for numerical stability to avoid divide by zero errors\n",
    "    \n",
    "    # References\n",
    "        V-Net: Fully Convolutional Neural Networks for Volumetric Medical Image Segmentation \n",
    "        https://arxiv.org/abs/1606.04797\n",
    "        More details on Dice loss formulation \n",
    "        https://mediatum.ub.tum.de/doc/1395260/1395260.pdf (page 72)\n",
    "        \n",
    "        Adapted from https://github.com/Lasagne/Recipes/issues/99#issuecomment-347775022\n",
    "    '''\n",
    "    \n",
    "    # skip the batch and class axis for calculating Dice score\n",
    "    axes = tuple(range(1, len(y_pred.shape)-1)) \n",
    "    numerator = 2. * K.sum(y_pred * y_true, axes)\n",
    "    denominator = K.sum(K.square(y_pred) + K.square(y_true), axes)\n",
    "    \n",
    "    return 1 - K.mean(numerator / (denominator + epsilon)) # average over classes and batch\n",
    "\n",
    "def focal_loss(target, output, gamma=2):\n",
    "    # https://github.com/keras-team/keras/issues/6261#issuecomment-358826560\n",
    "    output /= K.sum(output, axis=-1, keepdims=True)\n",
    "    eps = K.epsilon()\n",
    "    output = K.clip(output, eps, 1. - eps)\n",
    "    return -K.sum(K.pow(1. - output, gamma) * target * K.log(output), axis=-1)\n",
    "\n",
    "def conv_norm(inp, filters, conv=Conv2D, kernel_size=3):\n",
    "    c = conv(filters=filters, kernel_size=kernel_size, padding='same')(inp)\n",
    "    c = BatchNormalization()(c)\n",
    "    return LeakyReLU(0.1)(c)\n",
    "\n",
    "def encode(inp, filters):\n",
    "    c = conv_norm(inp, filters)\n",
    "    c = conv_norm(c, filters)\n",
    "    c = conv_norm(c, filters)\n",
    "    p = MaxPooling2D(pool_size=2)(c)\n",
    "    return c, p\n",
    "\n",
    "def decode(inp, shortcut, filters):\n",
    "    up = concatenate([Conv2DTranspose(filters, 2, strides=2, padding='same')(inp), shortcut], axis=3)\n",
    "    c = BatchNormalization()(up)\n",
    "    c = conv_norm(c, filters)\n",
    "    return conv_norm(c, filters)\n",
    "\n",
    "filters_encode_decode = [16,32,32,64,128]\n",
    "filters_middle = [256, 256, 256]\n",
    "\n",
    "layers = [Input(params['dim'] + (1,))]\n",
    "for i, filters in enumerate(filters_encode_decode):\n",
    "    c, p = encode(layers[-1], filters)\n",
    "    layers.append(c)\n",
    "    layers.append(p)\n",
    "\n",
    "for i, filters in enumerate(filters_middle):\n",
    "    layers.append(conv_norm(layers[-1], filters))\n",
    "\n",
    "for i, filters in enumerate(reversed(filters_encode_decode)):\n",
    "    layers.append(decode(layers[-1], layers[((len(filters_encode_decode) - i) * 2) - 1], filters))\n",
    "\n",
    "layers.append(Conv2D(8, (1, 1), activation='sigmoid')(layers[-1]))\n",
    "\n",
    "model = Model(inputs=[layers[0]], outputs=[layers[-1]])\n",
    "model.compile(optimizer = RMSprop(lr=1e03), loss = focal_loss, metrics=['accuracy'])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_model = \"m1.h5\"\n",
    "checkpoint = ModelCheckpoint(current_model,save_best_only=True, verbose=1, monitor=\"val_loss\")\n",
    "reduce = ReduceLROnPlateau(factor=0.1, patience=5, monitor='val_loss')\n",
    "earlyStopping = EarlyStopping(patience=20, verbose=1,monitor=\"val_loss\")\n",
    "history = model.fit_generator(\n",
    "    generator=training_generator,\n",
    "    validation_data=validation_generator,\n",
    "    use_multiprocessing=True, # Only works if training data is loaded into RAM from HDF\n",
    "    workers=8,\n",
    "    callbacks=[earlyStopping, checkpoint, reduce],\n",
    "    epochs=10000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for imgs, masks in training_generator:\n",
    "    for img, mask in zip(imgs, masks):\n",
    "        pred = model.predict(np.asarray([img]))\n",
    "        \n",
    "        # plot with various axes scales\n",
    "        plt.figure(figsize=(20, 20))\n",
    "\n",
    "        plt.subplot(121)\n",
    "        plt.imshow(img.astype(np.uint8)[...,0], cmap='gray')\n",
    "\n",
    "        plt.subplot(122)\n",
    "        plt.imshow(pred[0][...,2])\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "index = 0\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    " \n",
    "# Read Image\n",
    "img = np.array(dataset.get_image(index))\n",
    "q, r = dataset.get_pose(index)\n",
    "xa, ya, visible = project(q, r)\n",
    "size = img.shape\n",
    "\n",
    "model_points_all, _ = getSatelliteModel()\n",
    "image_points_all = np.stack((xa, ya), axis=1)\n",
    "\n",
    "model_points = model_points_all[visible]\n",
    "image_points = image_points_all[visible]\n",
    "print(visible)\n",
    "print(model_points)\n",
    "print(image_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.transform import Rotation as R\n",
    " \n",
    "(success, rotation_vector, translation_vector) = cv2.solvePnP(model_points, image_points, Camera.K, None)\n",
    "\n",
    "print(q, r)\n",
    "rot = np.zeros((3, 3), dtype=np.float)\n",
    "#cv2.Rodrigues(rotation_vector, rot)\n",
    "print(R.from_rotvec(rotation_vector[...,0]).as_quat(), translation_vector[...,0])\n",
    "#print(R.from_dcm(rot).as_euler('zyx', degrees=True), R.from_dcm(rot).as_euler('zyx', degrees=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = 4\n",
    "cols = 2\n",
    "\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(12, 12))\n",
    "for i in range(rows):\n",
    "    for j in range(cols):\n",
    "        dataset.visualize(i * rows + j, ax=axes[i][j])\n",
    "        axes[i][j].axis('off')\n",
    "fig.tight_layout() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project a 3D point (0, 0, 1000.0) onto the image plane.\n",
    "# We use this to draw a line sticking out of the nose\n",
    " \n",
    " \n",
    "(nose_end_point2D, jacobian) = cv2.projectPoints(np.array([(1.0, 1.0, 1.0)]), rotation_vector, translation_vector, Camera.K, None)\n",
    " \n",
    "for p in image_points:\n",
    "    cv2.circle(img, (int(p[0]), int(p[1])), 5, (0,0,255), -1)\n",
    " \n",
    " \n",
    "p1 = ( int(image_points[0][0]), int(image_points[0][1]))\n",
    "p2 = ( int(nose_end_point2D[0][0][0]), int(nose_end_point2D[0][0][1]))\n",
    " \n",
    "cv2.line(img, p1, p2, (255,0,0), 2)\n",
    " \n",
    "# Display image\n",
    "plt.figure(figsize=(20,20))\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
