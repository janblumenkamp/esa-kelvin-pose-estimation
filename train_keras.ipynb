{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pose detection for the ESA Pose Estimation Challenge\n",
    "\n",
    "https://kelvins.esa.int/satellite-pose-estimation-challenge/problem/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1' # 1: RTX, 0: Titan\n",
    "import tensorflow as tf\n",
    "from imgaug import augmenters as iaa\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from submission import SubmissionWriter\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Input, Activation, AveragePooling2D, concatenate, Conv2D, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, TensorBoard\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.applications.vgg19 import preprocess_input\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing import image as keras_image\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from time import time\n",
    "from pyrr import Quaternion\n",
    "from PIL import Image\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure TensorFlow so that not all the GPU memory is allocated for this\n",
    "config = tf.ConfigProto(allow_soft_placement=True)\n",
    "config.gpu_options.allow_growth = True\n",
    "K.set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Camera:\n",
    "\n",
    "    \"\"\"\" Utility class for accessing camera parameters. \"\"\"\n",
    "\n",
    "    fx = 0.0176  # focal length[m]\n",
    "    fy = 0.0176  # focal length[m]\n",
    "    nu = 1920  # number of horizontal[pixels]\n",
    "    nv = 1200  # number of vertical[pixels]\n",
    "    ppx = 5.86e-6  # horizontal pixel pitch[m / pixel]\n",
    "    ppy = ppx  # vertical pixel pitch[m / pixel]\n",
    "    fpx = fx / ppx  # horizontal focal length[pixels]\n",
    "    fpy = fy / ppy  # vertical focal length[pixels]\n",
    "    k = [[fpx,   0, nu / 2],\n",
    "         [0,   fpy, nv / 2],\n",
    "         [0,     0,      1]]\n",
    "    K = np.array(k)\n",
    "\n",
    "\n",
    "def process_json_dataset(root_dir):\n",
    "    with open(os.path.join(root_dir, 'train.json'), 'r') as f:\n",
    "        train_images_labels = json.load(f)\n",
    "\n",
    "    with open(os.path.join(root_dir, 'test.json'), 'r') as f:\n",
    "        test_image_list = json.load(f)\n",
    "\n",
    "    with open(os.path.join(root_dir, 'real_test.json'), 'r') as f:\n",
    "        real_test_image_list = json.load(f)\n",
    "\n",
    "    partitions = {'test': [], 'train': [], 'real_test': []}\n",
    "    labels = {}\n",
    "\n",
    "    for image_ann in train_images_labels:\n",
    "        partitions['train'].append(image_ann['filename'])\n",
    "        labels[image_ann['filename']] = {'q': image_ann['q_vbs2tango'], 'r': image_ann['r_Vo2To_vbs_true']}\n",
    "\n",
    "    for image in test_image_list:\n",
    "        partitions['test'].append(image['filename'])\n",
    "\n",
    "    for image in real_test_image_list:\n",
    "        partitions['real_test'].append(image['filename'])\n",
    "\n",
    "    return partitions, labels\n",
    "\n",
    "\n",
    "def quat2dcm(q):\n",
    "\n",
    "    \"\"\" Computing direction cosine matrix from quaternion, adapted from PyNav. \"\"\"\n",
    "\n",
    "    # normalizing quaternion\n",
    "    q = q/np.linalg.norm(q)\n",
    "\n",
    "    q0 = q[0]\n",
    "    q1 = q[1]\n",
    "    q2 = q[2]\n",
    "    q3 = q[3]\n",
    "\n",
    "    dcm = np.zeros((3, 3))\n",
    "\n",
    "    dcm[0, 0] = 2 * q0 ** 2 - 1 + 2 * q1 ** 2\n",
    "    dcm[1, 1] = 2 * q0 ** 2 - 1 + 2 * q2 ** 2\n",
    "    dcm[2, 2] = 2 * q0 ** 2 - 1 + 2 * q3 ** 2\n",
    "\n",
    "    dcm[0, 1] = 2 * q1 * q2 + 2 * q0 * q3\n",
    "    dcm[0, 2] = 2 * q1 * q3 - 2 * q0 * q2\n",
    "\n",
    "    dcm[1, 0] = 2 * q1 * q2 - 2 * q0 * q3\n",
    "    dcm[1, 2] = 2 * q2 * q3 + 2 * q0 * q1\n",
    "\n",
    "    dcm[2, 0] = 2 * q1 * q3 + 2 * q0 * q2\n",
    "    dcm[2, 1] = 2 * q2 * q3 - 2 * q0 * q1\n",
    "\n",
    "    return dcm\n",
    "\n",
    "def pointInTriangle(t, p):\n",
    "    #https://stackoverflow.com/questions/2049582/how-to-determine-if-a-point-is-in-a-2d-triangle\n",
    "    a = 0.5 *(-t[1][1]*t[2][0] + t[0][1]*(-t[1][0] + t[2][0]) + t[0][0]*(t[1][1] - t[2][1]) + t[1][0]*t[2][1]);\n",
    "    s = 1/(2*a)*(t[0][1]*t[2][0] - t[0][0]*t[2][1] + (t[2][1] - t[0][1])*p[0] + (t[0][0] - t[2][0])*p[1]);\n",
    "    u = 1/(2*a)*(t[0][0]*t[1][1] - t[0][1]*t[1][0] + (t[0][1] - t[1][1])*p[0] + (t[1][0] - t[0][0])*p[1]);\n",
    "    return s > 0 and u > 0 and (1-s-u) > 0\n",
    "\n",
    "class Plane:\n",
    "    def __init__(self, points):\n",
    "        if len(points) != 3:\n",
    "            raise ValueError(\"Plane always consists of three points\")\n",
    "\n",
    "        self.points = np.asarray(points)\n",
    "        n = np.cross(points[1] - points[0], points[2] - points[0])\n",
    "        self.normal = n / np.linalg.norm(n, 2)\n",
    "\n",
    "    def intersect(self, v):\n",
    "        ndotu = self.normal.dot(v)\n",
    "        if abs(ndotu) < 1e-6:\n",
    "            raise ValueError(\"Line is parallel to plane\")\n",
    "\n",
    "        return -self.points[0] - (self.normal.dot(-self.points[0]) / ndotu) * v + self.points[0]\n",
    "\n",
    "    def intersects(self, v):\n",
    "        # First calculate intersection point of vector and this plane:\n",
    "        try:\n",
    "            intersection = self.intersect(v)\n",
    "        except ValueError:\n",
    "            # The vector is parallel to the line, so always return false (could be completely on it or completely off)\n",
    "            return False\n",
    "\n",
    "        # We have a rotated 3D plane (i.e. z coordinates are level) and want to remove the\n",
    "        # z coordinates while keeping relations (i.e. project 3D plane to xy-plane)\n",
    "        # https://stackoverflow.com/questions/1023948/rotate-normal-vector-onto-axis-plane\n",
    "        zAxisNew = self.normal\n",
    "        xAxisOld = np.array([1,0,0])\n",
    "        if np.array_equal(np.absolute(zAxisNew), xAxisOld):\n",
    "            # the old x axis cannot be the same as the normal (the new z axis) since then the\n",
    "            # coordinate system is perpendicular to the xy plane. Therefore change x and z then\n",
    "            xAxisOld = np.array([0,0,1])\n",
    "        yAxisOld = np.array([0,1,0])\n",
    "        yAxisNew = np.cross(xAxisOld, zAxisNew)\n",
    "        xAxisNew = np.cross(zAxisNew, yAxisNew)\n",
    "        yAxisNew /= np.linalg.norm(yAxisNew, 2)\n",
    "        xAxisNew /= np.linalg.norm(xAxisNew, 2)\n",
    "        projected2dtriangle = np.asarray([[p.dot(xAxisNew), p.dot(yAxisNew)] for p in self.points])\n",
    "        # Now we know the 2d projection of the points of the polygon. Also project the 3d intersection point to the same plane\n",
    "        projected2dpoint = np.asarray([intersection.dot(xAxisNew), intersection.dot(yAxisNew)])\n",
    "        return intersection, pointInTriangle(projected2dtriangle, projected2dpoint)\n",
    "\n",
    "def getSatelliteModel():\n",
    "    b = 0.6\n",
    "    a = 0.75\n",
    "    d = 0.8\n",
    "    c = 0.32\n",
    "\n",
    "    #     0         1\n",
    "    #     +---a-----+\n",
    "    #  d-/|   u    /|-c\n",
    "    # 3 +---------+ | 2\n",
    "    #   |w| y  z  |x|     (y: front, z: back)\n",
    "    # 4 | +-------|-+ 5\n",
    "    #   |/ v (0,0)|/-b\n",
    "    # 7 +---------+ 6\n",
    "    # reference points in satellite frame for drawing axes\n",
    "    return np.array([\n",
    "        [-a / 2,  d / 2, c], # 0\n",
    "        [ a / 2,  d / 2, c], # 1\n",
    "        [ a / 2, -d / 2, c], # 2\n",
    "        [-a / 2, -d / 2, c], # 3\n",
    "        [-a / 2,  b / 2, 0], # 4\n",
    "        [ a / 2,  b / 2, 0], # 5\n",
    "        [ a / 2, -b / 2, 0], # 6\n",
    "        [-a / 2, -b / 2, 0]  # 7\n",
    "    ]), np.array([\n",
    "        [0, 1, 2], [0, 3, 2], # u\n",
    "        [4, 5, 6], [4, 7, 6], # v\n",
    "        [0, 3, 7], [0, 4, 7], # w\n",
    "        [1, 2, 6], [1, 5, 6], # x\n",
    "        [3, 2, 6], [3, 7, 6], # y\n",
    "        [0, 1, 5], [0, 4, 5], # z\n",
    "    ])\n",
    "\n",
    "def projectModel(q, r, plot=False):\n",
    "    \"\"\"\n",
    "    Projecting points to image frame to draw axes\n",
    "    # 1) Determine 8 vertice points\n",
    "    # 2) Determine corresponding 8 surface planes\n",
    "    # 3) Determine 8 vectors between camera and current 3d point (vertitice)\n",
    "    # 4) Check if the 8 vectors intersect any of the 8 surface planes. If so, discard point\n",
    "    \"\"\"\n",
    "    model_coordinates, cube_polygon_indices = getSatelliteModel()\n",
    "    p_axes = np.ones((model_coordinates.shape[0], model_coordinates.shape[1] + 1))\n",
    "    p_axes[:,:-1] = model_coordinates\n",
    "    points_body = np.transpose(p_axes)\n",
    "\n",
    "    # transformation to camera frame\n",
    "    pose_mat = np.hstack((np.transpose(quat2dcm(q)), np.expand_dims(r, 1)))\n",
    "    p_cam = np.dot(pose_mat, points_body)\n",
    "\n",
    "    # Indices of points describing 3 point triangles of the cube\n",
    "    # No point should intersect any of these triangles to be visible in the camera\n",
    "\n",
    "    if plot:\n",
    "        fig = plt.figure()\n",
    "        ax = Axes3D(fig)\n",
    "\n",
    "    points_camera_t = p_cam.transpose()\n",
    "    points_camera_collision_indices = []\n",
    "    for polygon_indices in cube_polygon_indices:\n",
    "        points_polygon = points_camera_t[polygon_indices]\n",
    "        plane = Plane(points_polygon)\n",
    "\n",
    "        if plot:\n",
    "            tri = Axes3D.art3d.Poly3DCollection([plane.points], alpha=0.2)\n",
    "            tri.set_color([1,0,0])\n",
    "            tri.set_edgecolor('k')\n",
    "            ax.add_collection3d(tri)\n",
    "\n",
    "        for i, p in enumerate(points_camera_t):\n",
    "            intersection, intersects = plane.intersects(p)\n",
    "            if(intersects):\n",
    "                # The vector between camera origin and cube vertice intersects any of the 12 cube polygons.\n",
    "                # There are two border cases to check:\n",
    "                # 1) Sometimes an actual vertice intersects a neighboring polygon\n",
    "                # 2) The vector between camera and point intersects a polygon that actually is behind the point\n",
    "                dist_intersection = np.linalg.norm(intersection, 2)\n",
    "                dist_point = np.linalg.norm(p, 2)\n",
    "                if abs(dist_intersection - dist_point) > 0.01 and dist_intersection < dist_point and not i in points_camera_collision_indices:\n",
    "                    points_camera_collision_indices.append(i)\n",
    "                    if plot:\n",
    "                        ax.scatter([intersection[0]], [intersection[1]], [intersection[2]])\n",
    "\n",
    "    visible_points = np.ones(len(p_axes), dtype=bool)\n",
    "    visible_points[points_camera_collision_indices] = False\n",
    "\n",
    "    if plot:\n",
    "        for p in points_camera_t[visible_points]:\n",
    "            ax.plot([0, p[0]], [0, p[1]], [0, p[2]])\n",
    "\n",
    "        #ax.set_xlim(-1, 1)\n",
    "        #ax.set_ylim(-1, 1)\n",
    "        #ax.set_zlim(5, 7)\n",
    "        ax.autoscale()\n",
    "        ax.set_xlabel('X axis')\n",
    "        ax.set_ylabel('Y axis')\n",
    "        ax.set_zlabel('Z axis')\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "    p_cam = points_camera_t.transpose()\n",
    "\n",
    "    # getting homogeneous coordinates\n",
    "    points_camera_frame = p_cam / p_cam[2]\n",
    "    # projection to image plane\n",
    "    points_image_plane = Camera.K.dot(points_camera_frame)\n",
    "\n",
    "    x, y = (points_image_plane[0], points_image_plane[1])\n",
    "    return x, y, visible_points\n",
    "\n",
    "def projectAxes(q, r):\n",
    "\n",
    "    \"\"\" Projecting points to image frame to draw axes \"\"\"\n",
    "\n",
    "    # reference points in satellite frame for drawing axes\n",
    "    p_axes = np.array([[0, 0, 0, 1],\n",
    "                       [1, 0, 0, 1],\n",
    "                       [0, 1, 0, 1],\n",
    "                       [0, 0, 1, 1]])\n",
    "    points_body = np.transpose(p_axes)\n",
    "\n",
    "    # transformation to camera frame\n",
    "    pose_mat = np.hstack((np.transpose(quat2dcm(q)), np.expand_dims(r, 1)))\n",
    "    p_cam = np.dot(pose_mat, points_body)\n",
    "\n",
    "    # getting homogeneous coordinates\n",
    "    points_camera_frame = p_cam / p_cam[2]\n",
    "\n",
    "    # projection to image plane\n",
    "    points_image_plane = Camera.K.dot(points_camera_frame)\n",
    "\n",
    "    x, y = (points_image_plane[0], points_image_plane[1])\n",
    "    return x, y\n",
    "\n",
    "\n",
    "class SatellitePoseEstimationDataset:\n",
    "\n",
    "    \"\"\" Class for dataset inspection: easily accessing single images, and corresponding ground truth pose data. \"\"\"\n",
    "\n",
    "    def __init__(self, root_dir='/datasets/speed_debug'):\n",
    "        self.partitions, self.labels = process_json_dataset(root_dir)\n",
    "        self.root_dir = root_dir\n",
    "\n",
    "    def get_image(self, i=0, split='train'):\n",
    "\n",
    "        \"\"\" Loading image as PIL image. \"\"\"\n",
    "\n",
    "        img_name = self.partitions[split][i]\n",
    "        img_name = os.path.join(self.root_dir, 'images', split, img_name)\n",
    "        image = Image.open(img_name).convert('RGB')\n",
    "        return image\n",
    "\n",
    "    def get_pose(self, i=0):\n",
    "\n",
    "        \"\"\" Getting pose label for image. \"\"\"\n",
    "\n",
    "        img_id = self.partitions['train'][i]\n",
    "        q, r = self.labels[img_id]['q'], self.labels[img_id]['r']\n",
    "        return q, r\n",
    "\n",
    "    def visualize(self, i, partition='train', ax=None):\n",
    "\n",
    "        \"\"\" Visualizing image, with ground truth pose with axes projected to training image. \"\"\"\n",
    "\n",
    "        if ax is None:\n",
    "            ax = plt.gca()\n",
    "        img = self.get_image(i)\n",
    "        ax.imshow(img)\n",
    "\n",
    "        # no pose label for test\n",
    "        if partition == 'train':\n",
    "            q, r = self.get_pose(i)\n",
    "            xa, ya = projectAxes(q, r)\n",
    "            ax.arrow(xa[0], ya[0], xa[1] - xa[0], ya[1] - ya[0], head_width=30, color='r')\n",
    "            ax.arrow(xa[0], ya[0], xa[2] - xa[0], ya[2] - ya[0], head_width=30, color='g')\n",
    "            ax.arrow(xa[0], ya[0], xa[3] - xa[0], ya[3] - ya[0], head_width=30, color='b')\n",
    "\n",
    "        return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset via API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_root_dir = './speed'\n",
    "dataset = SatellitePoseEstimationDataset(root_dir=dataset_root_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize a few images from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, 5):\n",
    "    img = np.array(dataset.get_image(i))\n",
    "    \n",
    "    fig,ax = plt.subplots(1)\n",
    "    ax.imshow(img)\n",
    "\n",
    "    q, r = dataset.get_pose(i)\n",
    "    xa, ya, visible = projectModel(q, r)\n",
    "    for x, y, v in zip(xa, ya, visible):\n",
    "        if v and x >= 0.0 and y >= 0.0 and x <= Camera.nu and y <= Camera.nv:\n",
    "            ax.add_patch(Rectangle((x, y),3,3,linewidth=5,edgecolor='r',facecolor='none'))\n",
    "\n",
    "    dataset.visualize(i, ax=ax)\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'dim': (600, 960),\n",
    "    'batch_size': 1,\n",
    "    'label_size': 1.5,\n",
    "    'shuffle': True,\n",
    "    'output_scale': 8,\n",
    "    'n_output_vertices': 8,\n",
    "    'stages': 6,\n",
    "    'augmentation': False\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network definition\n",
    "### Option 1: Define model for training from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use pre-trained VGG19 convolutional layer as first stage\n",
    "pretrained_model = tf.keras.applications.vgg19.VGG19(\n",
    "    weights=\"imagenet\",\n",
    "    include_top=False,\n",
    "    input_shape=config['dim'] + (3,)\n",
    ")\n",
    "\n",
    "# Network architecture according to https://arxiv.org/abs/1602.00134\n",
    "def create_stage(inp, filters):\n",
    "    # Stage definition according to CPM Paper\n",
    "    c = Conv2D(filters=64, kernel_size=7, strides=1, padding=\"same\")(inp)\n",
    "    c = BatchNormalization()(c)\n",
    "    c = Activation(\"relu\")(c)\n",
    "    \n",
    "    for i in range(4):\n",
    "        c = Conv2D(filters=64, kernel_size=7, strides=1, padding=\"same\")(c)\n",
    "        c = BatchNormalization()(c)\n",
    "        c = Activation(\"relu\")(c)\n",
    "\n",
    "    c = Conv2D(filters=64, kernel_size=1, strides=1, padding=\"same\")(c)\n",
    "    c = BatchNormalization()(c)\n",
    "    c = Activation(\"relu\")(c)\n",
    "    return Conv2D(filters=filters, kernel_size=1, strides=1, padding=\"same\")(c)\n",
    "    \n",
    "# Adding new trainable hidden and output layers to the model\n",
    "\n",
    "inp = pretrained_model.input\n",
    "inp_avg = AveragePooling2D(pool_size=9, strides=8, padding=\"same\")(inp)\n",
    "\n",
    "c = pretrained_model.layers[13].output\n",
    "c = Conv2D(filters=128, kernel_size=3, strides=1, padding=\"same\")(c)\n",
    "c = BatchNormalization()(c)\n",
    "c = Activation(\"relu\")(c)\n",
    "\n",
    "pre = Conv2D(filters=128, kernel_size=3, strides=1, padding=\"same\")(c)\n",
    "pre = BatchNormalization()(pre)\n",
    "pre = Activation(\"relu\")(pre)\n",
    "\n",
    "s1 = Conv2D(filters=128, kernel_size=1, strides=1, padding=\"same\")(pre)\n",
    "stages = [Conv2D(filters=config['n_output_vertices'], kernel_size=1, strides=1, padding=\"same\")(s1)]\n",
    "\n",
    "for i in range(5):\n",
    "    stages.append(create_stage(concatenate([pre, stages[-1], inp_avg], axis=3), config['n_output_vertices']))\n",
    "\n",
    "model = tf.keras.models.Model(inputs=inp, outputs=stages)\n",
    "model.compile(loss=\"mse\", optimizer=Adam(lr=1e-3))\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2: Load trained model for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(\"model.h5\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data generator definition and declaration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KerasDataGenerator(Sequence):\n",
    "\n",
    "    \"\"\" DataGenerator for Keras to be used with fit_generator (https://keras.io/models/sequential/#fit_generator)\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 preprocessor,\n",
    "                 label_list,\n",
    "                 speed_root,\n",
    "                 label_size,\n",
    "                 batch_size,\n",
    "                 dim,\n",
    "                 shuffle=True,\n",
    "                 output_scale=8,\n",
    "                 n_output_vertices=8,\n",
    "                 stages=6,\n",
    "                augmentation=True):\n",
    "\n",
    "        # loading dataset\n",
    "        self.image_root = speed_root\n",
    "\n",
    "        # Initialization\n",
    "        self.preprocessor = preprocessor\n",
    "        self.dim = dim\n",
    "        self.batch_size = batch_size\n",
    "        self.labels = {label['filename']: {'q': label['q_vbs2tango'], 'r': label['r_Vo2To_vbs_true']} for label in label_list}\n",
    "        self.list_IDs = [label['filename'] for label in label_list]\n",
    "        self.shuffle = shuffle\n",
    "        self.label_size = label_size\n",
    "        self.indexes = None\n",
    "        self.output_scale = output_scale\n",
    "        self.n_output_vertices = n_output_vertices\n",
    "        self.stages = stages\n",
    "        self.augmentation = augmentation\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        \"\"\" Denotes the number of batches per epoch. \"\"\"\n",
    "\n",
    "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        \"\"\" Generate one batch of data \"\"\"\n",
    "\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "\n",
    "        # Generate data\n",
    "        return self.__data_generation(list_IDs_temp)\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "\n",
    "        \"\"\" Updates indexes after each epoch \"\"\"\n",
    "\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def drawBlob(self, img, pos, sigma=3):\n",
    "        # https://github.com/NVlabs/Deep_Object_Pose/blob/master/src/training/train.py#L851\n",
    "        w = int(sigma*3)\n",
    "        if pos[0]-w>=0 and pos[0]+w<img.shape[0] and pos[1]-w>=0 and pos[1]+w<img.shape[1]:\n",
    "            for i in range(int(pos[0])-w, int(pos[0])+w):\n",
    "                for j in range(int(pos[1])-w, int(pos[1])+w):\n",
    "                    img[i,j] = np.exp(-(((i - pos[0])**2 + (j - pos[1])**2)/(2*(sigma**2))))\n",
    "\n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "\n",
    "        \"\"\" Generates data containing batch_size samples \"\"\"\n",
    "\n",
    "        # Initialization\n",
    "        imgs = np.empty((self.batch_size, *self.dim, 3))\n",
    "        masks = np.zeros(\n",
    "            (self.batch_size,\n",
    "             int(self.dim[0] / self.output_scale),\n",
    "             int(self.dim[1] / self.output_scale),\n",
    "             self.n_output_vertices\n",
    "            ), dtype=np.float)\n",
    "\n",
    "        seq = iaa.SomeOf((0, 3),\n",
    "            [\n",
    "                iaa.OneOf([\n",
    "                    iaa.GaussianBlur((0, 3.0)), # blur images with a sigma between 0 and 3.0\n",
    "                    iaa.AverageBlur(k=(2, 7)), # blur image using local means with kernel sizes between 2 and 7\n",
    "                    #iaa.MedianBlur(k=(3, 11)), # blur image using local medians with kernel sizes between 2 and 7\n",
    "                ]),\n",
    "                iaa.Sharpen(alpha=(0, 1.0), lightness=(0.75, 1.5)), # sharpen images\n",
    "                # search either for all edges or for directed edges,\n",
    "                # blend the result with the original image using a blobby mask\n",
    "                iaa.SimplexNoiseAlpha(iaa.OneOf([\n",
    "                    iaa.EdgeDetect(alpha=(0.5, 1.0)),\n",
    "                    iaa.DirectedEdgeDetect(alpha=(0.5, 1.0), direction=(0.0, 1.0)),\n",
    "                ])),\n",
    "                iaa.AdditiveGaussianNoise(loc=0, scale=(0.0, 0.05*255)), # add gaussian noise to images\n",
    "                iaa.OneOf([\n",
    "                    iaa.Dropout((0.01, 0.1)), # randomly remove up to 10% of the pixels\n",
    "                    iaa.CoarseDropout((0.03, 0.15), size_percent=(0.02, 0.05)),\n",
    "                ]),\n",
    "                iaa.Add((-10, 10)), # change brightness of images (by -10 to 10 of original value)\n",
    "                iaa.OneOf([\n",
    "                    iaa.Multiply((0.5, 1.5)),\n",
    "                    iaa.FrequencyNoiseAlpha(\n",
    "                        exponent=(-4, 0),\n",
    "                        first=iaa.Multiply((0.5, 1.5)),\n",
    "                        second=iaa.ContrastNormalization((0.5, 2.0))\n",
    "                    )\n",
    "                ]),\n",
    "                iaa.ContrastNormalization((0.5, 2.0)), # improve or worsen the contrast\n",
    "            ],\n",
    "            random_order=True\n",
    "        ).to_deterministic()\n",
    "\n",
    "        # Generate data\n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            img_path = os.path.join(self.image_root, ID)\n",
    "            img = keras_image.load_img(img_path, target_size=self.dim) #, color_mode = \"grayscale\")\n",
    "            img = keras_image.img_to_array(img)\n",
    "            if self.augmentation:\n",
    "                img = seq.augment_image(img)\n",
    "            imgs[i] = self.preprocessor(img)\n",
    "\n",
    "            q, r = self.labels[ID]['q'], self.labels[ID]['r']\n",
    "            xa, ya, visibles = projectModel(q, r)\n",
    "            for j, (x, y, visible) in enumerate(zip(xa, ya, visibles)):\n",
    "                if j >= self.n_output_vertices:\n",
    "                    break\n",
    "                \n",
    "                x /= (Camera.nu * self.output_scale)\n",
    "                y /= (Camera.nv * self.output_scale)\n",
    "                if visible and x >= 0.0 and y >= 0.0 and x <= 1.0 and y <= 1.0:\n",
    "                    x_s, y_s = int(x * self.dim[1]), int(y * self.dim[0])\n",
    "                    self.drawBlob(masks[i][...,j], (y_s, x_s), self.label_size)\n",
    "\n",
    "        return imgs, [masks*255 for i in range(self.stages)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load labels for training\n",
    "with open(os.path.join(dataset_root_dir, 'train.json'), 'r') as f:\n",
    "    label_list = json.load(f)\n",
    "\n",
    "# Split training validation 80/20\n",
    "train_labels = label_list[:int(len(label_list)*.8)]\n",
    "validation_labels = label_list[int(len(label_list)*.8):]\n",
    "\n",
    "# Data generators for training and validation\n",
    "training_generator = KerasDataGenerator(preprocess_input, train_labels, dataset_root_dir + \"/images/train\", **config)\n",
    "validation_generator = KerasDataGenerator(preprocess_input, validation_labels, dataset_root_dir + \"/images/train\", **config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A data generator can also be created for the real data, but this is currently not used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(dataset_root_dir, 'real.json'), 'r') as f:\n",
    "    real_label_list = json.load(f)\n",
    "\n",
    "real_generator = KerasDataGenerator(preprocess_input, real_label_list, \"./speed/images/real\", **config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify the data generator by printing the scaled image and the corresponding keypoint heatmap (all n output heatmaps for each of the vertices is merged to one heatmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for imgs, masks in training_generator:\n",
    "    print(imgs.shape, len(masks), masks[0].shape)\n",
    "    for img, mask in zip(imgs, masks[0]):        \n",
    "        plt.figure(figsize=(20, 20))\n",
    "\n",
    "        plt.subplot(121)\n",
    "        plt.imshow(img[...,0], cmap='gray')\n",
    "\n",
    "        plt.subplot(122)\n",
    "        m = np.zeros(mask.shape[:2], dtype=np.float)\n",
    "        for i in range(config['n_output_vertices']):\n",
    "            m += mask[...,i]\n",
    "        plt.imshow(m, cmap='gray')\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint(\"model_.h5\",save_best_only=True, verbose=1, monitor=\"val_loss\")\n",
    "reduce = ReduceLROnPlateau(factor=0.5, patience=3, monitor='val_loss')\n",
    "earlyStopping = EarlyStopping(patience=6, verbose=1,monitor=\"val_loss\")\n",
    "tensorboard = TensorBoard(log_dir='logs/{}'.format(time()))\n",
    "history = model.fit_generator(\n",
    "    generator=training_generator,\n",
    "    validation_data=validation_generator,\n",
    "    use_multiprocessing=False,\n",
    "    workers=8,\n",
    "    callbacks=[earlyStopping, checkpoint, tensorboard],\n",
    "    epochs=10000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def points_to_pose(points_3d, points_2d, camera_matrix=Camera.K):\n",
    "    def convert_rvec_to_quaternion(rvec):\n",
    "        '''Convert rvec (which is log quaternion) to quaternion'''\n",
    "        theta = np.sqrt(rvec[0] * rvec[0] + rvec[1] * rvec[1] + rvec[2] * rvec[2])  # in radians\n",
    "        raxis = [rvec[0] / theta, rvec[1] / theta, rvec[2] / theta]\n",
    "\n",
    "        # pyrr's Quaternion (order is XYZW), https://pyrr.readthedocs.io/en/latest/oo_api_quaternion.html\n",
    "        return np.roll(Quaternion.from_axis_rotation(raxis, theta), 1) # change order to wxyz\n",
    "\n",
    "    (success, rotation_vector, translation_vector, outliners) = cv2.solvePnPRansac(\n",
    "        points_3d, points_2d, camera_matrix, None, iterationsCount=250, reprojectionError=50)\n",
    "    #(success, rotation_vector, translation_vector) = cv2.solvePnP(points_3d, points_2d, camera_matrix, None)\n",
    "    #print(outliners)\n",
    "    if success:\n",
    "        location = list(translation_vector[...,0])\n",
    "        quaternion = convert_rvec_to_quaternion(rotation_vector)\n",
    "\n",
    "        projected_points, _ = cv2.projectPoints(points_3d, rotation_vector, translation_vector, camera_matrix, None)\n",
    "        projected_points = np.squeeze(projected_points)\n",
    "\n",
    "        # If the location.Z is negative or object is behind the camera then flip both location and rotation\n",
    "        x, y, z = location\n",
    "        if z < 0:\n",
    "            print(\"neg\")\n",
    "            # Get the opposite location\n",
    "            location = [-x, -y, -z]\n",
    "\n",
    "            # Change the rotation by 180 degree\n",
    "            rotate_angle = np.pi\n",
    "            rotate_quaternion = Quaternion.from_axis_rotation(location, rotate_angle)\n",
    "            quaternion = rotate_quaternion.cross(quaternion)\n",
    "\n",
    "        return quaternion, location, projected_points\n",
    "    return [], [], []\n",
    "\n",
    "def extract_maxima(belief, area=5):\n",
    "    b = belief.copy()\n",
    "    maxima = []\n",
    "    maxima_vals = []\n",
    "    for i in range(3):\n",
    "        pmax = np.unravel_index(b.argmax(), b.shape)\n",
    "        pmax_val = b[pmax[0]][pmax[1]]\n",
    "        if i > 0 and pmax_val < maxima_vals[-1] * 0.5 or pmax_val < 30.0:\n",
    "        #if pmax_val < 100.0:\n",
    "            break\n",
    "        maxima_vals.append(pmax_val)\n",
    "        b[max(0,pmax[0]-area):min(b.shape[0], pmax[0]+area):, max(0,pmax[1]-area):min(b.shape[1], pmax[1]+area):] = 0\n",
    "        maxima.append(pmax)\n",
    "    return np.asarray(maxima)\n",
    "\n",
    "def predict_pose(model, img, mask=None, debug=True):\n",
    "    model_points_all, _ = getSatelliteModel()\n",
    "\n",
    "    pred = model.predict(np.asarray([img]))\n",
    "\n",
    "    if debug:\n",
    "        plt.figure(figsize=(20, 40))\n",
    "\n",
    "    points_3d = []\n",
    "    points_2d = []\n",
    "    for i, p3d in enumerate(model_points_all):\n",
    "        p = pred[-1][0][...,i]\n",
    "        if debug:\n",
    "            plt.subplot(8,3,i * 3 + 1)\n",
    "            plt.imshow((img + 128).astype(np.uint8), cmap='gray')\n",
    "            ax = plt.subplot(8,3,i * 3 + 2)\n",
    "            ax.imshow(p)\n",
    "        maxima = extract_maxima(p)\n",
    "        if debug and mask is not None:\n",
    "            axmask = plt.subplot(8,3,i * 3 + 3)\n",
    "            axmask.imshow(mask[...,i], vmin=0, vmax=255)\n",
    "        for m in maxima:\n",
    "            points_3d.append(p3d)\n",
    "            points_2d.append([m[1]*8*2, m[0]*8*2])\n",
    "            if debug:\n",
    "                ax.add_patch(Rectangle((m[1], m[0]),1,1,linewidth=5,edgecolor='r',facecolor='none'))\n",
    "                if mask is not None:\n",
    "                    axmask.add_patch(Rectangle((m[1], m[0]),1,1,linewidth=5,edgecolor='r',facecolor='none'))\n",
    "            #break\n",
    "    points_3d = np.array(points_3d, dtype=np.float32)\n",
    "    points_2d = np.array(points_2d, dtype=np.float32)\n",
    "    if len(points_2d) < 4:\n",
    "        return np.array([0]*4), np.array([0]*3)\n",
    "    \n",
    "    quat_res, trans_res, points = points_to_pose(points_3d, points_2d, Camera.K)\n",
    "    if debug:\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"orig\", points_2d)\n",
    "        print(\"model\", points_3d)\n",
    "        print(\"proj\", points)\n",
    "\n",
    "        aximgproj = plt.subplot(111)\n",
    "        aximgproj.imshow(np.clip(img + 128, 0, 255).astype(np.uint8), cmap='gray')\n",
    "        for p in points:\n",
    "            aximgproj.add_patch(Rectangle((p[0]/2, p[1]/2),3,3,linewidth=2,edgecolor='r',facecolor='none'))\n",
    "        \n",
    "        xa, ya = projectAxes(q, r)\n",
    "        aximgproj.arrow(xa[0], ya[0], xa[1] - xa[0], ya[1] - ya[0], head_width=30, color='r')\n",
    "        aximgproj.arrow(xa[0], ya[0], xa[2] - xa[0], ya[2] - ya[0], head_width=30, color='g')\n",
    "        aximgproj.arrow(xa[0], ya[0], xa[3] - xa[0], ya[3] - ya[0], head_width=30, color='b')\n",
    "\n",
    "        plt.show()\n",
    "    return quat_res, trans_res\n",
    "    #print(\"res\", quat_res, trans_res)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform a orientation determination on the validation data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for imgs, masks in validation_generator:\n",
    "    for img, mask in zip(imgs, masks[0]):        \n",
    "        try:\n",
    "            quat_res, trans_res = predict_pose(model, img, mask, True)\n",
    "        except cv2.error as e:\n",
    "            continue\n",
    "        print(\"res\", quat_res, trans_res)\n",
    "        print(\"=\"*30)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform orientation determination and save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataset, append_submission, dataset_root):\n",
    "\n",
    "    \"\"\" Running evaluation on test set, appending results to a submission. \"\"\"\n",
    "\n",
    "    with open(os.path.join(dataset_root, dataset + '.json'), 'r') as f:\n",
    "        image_list = json.load(f)\n",
    "\n",
    "    print('Running evaluation on {} set...'.format(dataset))\n",
    "\n",
    "    err1 = 0\n",
    "    err2 = 0\n",
    "    err3 = 0\n",
    "    err4 = 0\n",
    "    for i, img in enumerate(image_list):\n",
    "        print(\"index\", i)\n",
    "        img_path = os.path.join(dataset_root, 'images', dataset, img['filename'])\n",
    "        \n",
    "        img_raw = keras_image.load_img(img_path, target_size=(600, 960)) #, color_mode = \"grayscale\")\n",
    "        img_raw = keras_image.img_to_array(img_raw)\n",
    "        img_proc = preprocess_input(img_raw)\n",
    "        \n",
    "        try:\n",
    "            quat_res, trans_res = predict_pose(model, img_proc, debug=True)\n",
    "            print(quat_res, trans_res)\n",
    "            \n",
    "            img_orig = keras_image.load_img(img_path)\n",
    "            plt.imshow(img_orig)\n",
    "            xa, ya = projectAxes(np.array(quat_res), np.array(trans_res))\n",
    "            plt.arrow(xa[0], ya[0], xa[1] - xa[0], ya[1] - ya[0], head_width=30, color='r')\n",
    "            plt.arrow(xa[0], ya[0], xa[2] - xa[0], ya[2] - ya[0], head_width=30, color='g')\n",
    "            plt.arrow(xa[0], ya[0], xa[3] - xa[0], ya[3] - ya[0], head_width=30, color='b')\n",
    "            plt.show()\n",
    "            \n",
    "            if len(quat_res) == 0 or len(trans_res) == 0:\n",
    "                append_submission(img['filename'], [0]*4, [0,0,3])\n",
    "                err1 += 1\n",
    "            elif (np.array(trans_res) == 0).all():\n",
    "                append_submission(img['filename'], [0]*4, [0,0,10])\n",
    "                err4 += 1\n",
    "            elif trans_res[2] > 60:\n",
    "                append_submission(img['filename'], [0]*4, [0,0,40])\n",
    "                err2 += 1\n",
    "            else:\n",
    "                append_submission(img['filename'], quat_res, trans_res)\n",
    "        except cv2.error as e:\n",
    "            append_submission(img['filename'], [0]*4, [0,0,10])\n",
    "            err3 += 1\n",
    "        print(\"=\"*30)\n",
    "\n",
    "    print(\"Err amount\", err1, err2, err3, err4)\n",
    "            \n",
    "submission = SubmissionWriter()\n",
    "evaluate(model, 'test', submission.append_test, dataset_root_dir)\n",
    "evaluate(model, 'real_test', submission.append_real_test, dataset_root_dir)\n",
    "submission.export(suffix='submission')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
